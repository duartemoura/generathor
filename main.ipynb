{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b44629b8-b3c9-4783-8741-613656661bd9",
   "metadata": {},
   "source": [
    "# Main Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07439da-c162-4d6b-b6da-41f63b9a8680",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62f45f4-0ce5-4d70-8716-a536e8adbf5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-community\n",
    "!pip install faiss-gpu\n",
    "!pip install peft\n",
    "!pip install transformers\n",
    "!pip install ragas\n",
    "!pip install llama_index\n",
    "!pip install langchain-aws\n",
    "!pip install bitsandbytes\n",
    "!pip install trl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9ee5771-c8b4-4bdf-8e78-bcc1f8ac9617",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import yaml\n",
    "import boto3\n",
    "import src\n",
    "from src.document_processor.loader import DocumentLoader\n",
    "from src.document_processor.chunker import DocumentChunker\n",
    "from src.document_processor.cleaner import TextCleaner\n",
    "from src.embeddings.embedding_manager import EmbeddingManager\n",
    "from src.question_generation.generator import EnhancedQuestionGenerator\n",
    "import logging\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ragas\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from ragas.testset import TestsetGenerator\n",
    "from langchain_openai import ChatOpenAI\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import gc\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "# Import necessary modules\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import Faithfulness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a session using the default profile or environment credentials\n",
    "session = boto3.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb046e48-dedc-4722-bc0d-09d1f812faad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/home/ec2-user/SageMaker/Fine_Tune_LLMs/Big/config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=getattr(logging, config['logging']['level']),\n",
    "    format=config['logging']['format']\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize AWS client\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "# Initialize components\n",
    "loader = DocumentLoader()\n",
    "chunker = DocumentChunker(\n",
    "    chunk_size=config['document_processing']['chunk_size'],\n",
    "    chunk_overlap=config['document_processing']['chunk_overlap']\n",
    ")\n",
    "cleaner = TextCleaner()\n",
    "\n",
    "embedding_manager = EmbeddingManager(\n",
    "    bedrock_client,\n",
    "    model_id=config['embedding']['model_id']\n",
    ")\n",
    "\n",
    "generator = EnhancedQuestionGenerator(\n",
    "    llm_client=bedrock_client,\n",
    "    model_id=config['question_generation']['model_id'],\n",
    "    embedding_manager=embedding_manager,\n",
    "    max_tokens=config['question_generation']['max_tokens'],\n",
    "    temperature=config['question_generation']['temperature']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0386b-19bd-4f54-aebb-08cdae279a28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process documents\n",
    "documents = loader.load_document(\"/home/ec2-user/SageMaker/Fine_Tune_LLMs/Big/data/NM_changed.pdf\")\n",
    "logger.info(\"Documents loaded\")\n",
    "\n",
    "# Clean and chunk documents\n",
    "cleaned_documents = []\n",
    "for doc in documents:\n",
    "    doc.page_content = cleaner.clean_text(doc.page_content)\n",
    "    cleaned_documents.append(doc)\n",
    "logger.info(\"Documents cleaned\")\n",
    "\n",
    "chunks = chunker.chunk_documents(cleaned_documents)\n",
    "logger.info(f\"Documents chunked into {len(chunks)} chunks\")\n",
    "\n",
    "# Create embeddings\n",
    "embedding_manager.create_embeddings(chunks)\n",
    "logger.info(\"Embeddings created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c86859-442d-438f-8ee5-a6baecfe353e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create embeddings and save them\n",
    "#embedding_manager.create_embeddings(documents)\n",
    "embedding_manager.save_embeddings(\"faiss_index\", \"metadata.txt\")\n",
    "\n",
    "# Load the saved embeddings\n",
    "#embedding_manager.load_embeddings(\"faiss_index\")\n",
    "\n",
    "# Wipe the existing database\n",
    "#embedding_manager.wipe_embeddings(\"faiss_index\", \"metadata.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e19bd-3b20-4e4b-8329-33dcdbf08c35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate questions\n",
    "questions = generator.generate_questions_from_docs(\n",
    "    chunks,\n",
    "    num_questions= 100\n",
    ")\n",
    "logger.info(f\"{len(questions)} question-answer pairs generated\")\n",
    "\n",
    "# Prepare data for DataFrame\n",
    "qa_data = [{\"Question\": qa.question, \"Answer\": qa.answer, \"Context\": qa.context} for qa in questions]\n",
    "\n",
    "# Create DataFrame\n",
    "qa_df = pd.DataFrame(qa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe3e99d-1e39-4928-8333-25bb6754dd0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the DataFrame\n",
    "qa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd2fe6b-daf5-47ba-abad-53cd039426db",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df.to_csv(\"qa_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e5034-231b-4b45-9fe8-99185a34a0bf",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fefaf6-37b6-4a56-be64-a01bebc8fbd9",
   "metadata": {},
   "source": [
    "## Ministral-3B-instruct\n",
    "\n",
    "I'm going to test this model on really specific questions from the Nuclear Medicine manual and see how well it responds firs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450ff7f4-a0cd-4ce0-b36b-af5fb3368f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Memory management class\n",
    "class MemoryTracker:\n",
    "    def __init__(self):\n",
    "        self.records = []\n",
    "    \n",
    "    def log_memory(self, checkpoint):\n",
    "        memory_stats = {\n",
    "            'checkpoint': checkpoint,\n",
    "            'allocated': torch.cuda.memory_allocated() / 1024**2,\n",
    "            'reserved': torch.cuda.memory_reserved() / 1024**2\n",
    "        }\n",
    "        self.records.append(memory_stats)\n",
    "        return memory_stats\n",
    "\n",
    "    def clear_memory(self):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "memory_tracker = MemoryTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b83be09-c8a0-4cd3-95d5-4dffca916ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "# Run garbage collector\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e874e7e-bd71-4b74-85a1-5c44a910e944",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 0.00 MB\n",
      "Cached memory: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "def print_gpu_memory():\n",
    "    print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    print(f\"Cached memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f45104-1318-40db-93a5-698a732a0b39",
   "metadata": {},
   "source": [
    "## Generating answers based on the questions generated by Claude in the previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca80ff2-5902-4417-a311-e382696d2e5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"ministral/Ministral-3b-instruct\", device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7a338da-5abd-434f-aa60-0dd9d0b82dea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5396510352a4e32973d666365b9911f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47260672ab984aee969fbeb5ec275fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba60d2bedaec48cb9bf4f90422f0d45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9e09c2195744bc935ffaee37ed258a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/2.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e71239e6d3747cfb44b4ff2ee4d6908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/2.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c52b3ef5be241e0a42181a40517868b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/698M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17090d2bcdb94e5ea07364e736385154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9c8f9f870041f6980caf7929c9d753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8036dbad3154751825dc367e9834891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679091d98b9b47cbac7dc0943871a5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157fc7410a0748fb84cb2dfe9fe43764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/510 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "# Load questions from the CSV\n",
    "qa_df = pd.read_csv(\"qa_data.csv\")  # Replace with your CSV file path\n",
    "questions = qa_df['Question'].tolist()  # Assuming 'question' is the column name\n",
    "\n",
    "# Initialize the Mistral model pipeline\n",
    "pipe = pipeline(\"text-generation\", model=\"ministral/Ministral-3b-instruct\", device = 0)\n",
    "\n",
    "responses = []\n",
    "for question in questions:\n",
    "    messages = [{\"role\": \"user\", \"content\": question}]\n",
    "    response = pipe(messages, max_new_tokens=600, do_sample=True, temperature = 0.4, top_p = 0.7)\n",
    "\n",
    "    # Navigate the nested structure to extract the assistant's content\n",
    "    try:\n",
    "        generated_messages = response[0]['generated_text']\n",
    "        assistant_message = next(\n",
    "            (msg['content'] for msg in generated_messages if msg['role'] == 'assistant'), \n",
    "            \"No assistant response found\"\n",
    "        )\n",
    "        responses.append(assistant_message)\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError: {e} in response {response}\")\n",
    "        responses.append(\"Error: Unexpected response format\")\n",
    "\n",
    "# Combine questions and responses\n",
    "result_df = pd.DataFrame({'question': questions, 'mistral_response': responses})\n",
    "\n",
    "# Save the results to a new CSV\n",
    "result_df .to_csv(\"qa_with_mistral_responses.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a27464ba-04f3-4af1-9d78-33887bdcc812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_df = pd.read_csv(\"qa_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "753df524-ade3-4b6a-bda5-fe3394ebeaf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the fundamental concept and process in...</td>\n",
       "      <td>Nuclear medicine imaging involves administerin...</td>\n",
       "      <td>1 chapter What Is Nuclear Medicine?A. FUNDAMEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the current applications and global u...</td>\n",
       "      <td>As of 2006, there were roughly 100 different d...</td>\n",
       "      <td>1 chapter What Is Nuclear Medicine?A. FUNDAMEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the two broad classes of nuclear medi...</td>\n",
       "      <td>The two broad classes of nuclear medicine imag...</td>\n",
       "      <td>photons are emitted. The energy of these gamma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the key components of a gamma camera,...</td>\n",
       "      <td>The key components of a gamma camera are a col...</td>\n",
       "      <td>photons are emitted. The energy of these gamma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the two broad classes of nuclear medi...</td>\n",
       "      <td>The two broad classes of nuclear medicine imag...</td>\n",
       "      <td>particular angle. This results in an image wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  What is the fundamental concept and process in...   \n",
       "1  What are the current applications and global u...   \n",
       "2  What are the two broad classes of nuclear medi...   \n",
       "3  What are the key components of a gamma camera,...   \n",
       "4  What are the two broad classes of nuclear medi...   \n",
       "\n",
       "                                              Answer  \\\n",
       "0  Nuclear medicine imaging involves administerin...   \n",
       "1  As of 2006, there were roughly 100 different d...   \n",
       "2  The two broad classes of nuclear medicine imag...   \n",
       "3  The key components of a gamma camera are a col...   \n",
       "4  The two broad classes of nuclear medicine imag...   \n",
       "\n",
       "                                             Context  \n",
       "0  1 chapter What Is Nuclear Medicine?A. FUNDAMEN...  \n",
       "1  1 chapter What Is Nuclear Medicine?A. FUNDAMEN...  \n",
       "2  photons are emitted. The energy of these gamma...  \n",
       "3  photons are emitted. The energy of these gamma...  \n",
       "4  particular angle. This results in an image wit...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3545695f-646a-4202-9876-b5523d4ca8e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-aws\n",
      "  Downloading langchain_aws-0.2.7-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: boto3>=1.34.131 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-aws) (1.35.54)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3.15 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-aws) (0.3.19)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-aws) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-aws) (2.9.2)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.54 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3>=1.34.131->langchain-aws) (1.35.54)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3>=1.34.131->langchain-aws) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3>=1.34.131->langchain-aws) (0.10.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3.15->langchain-aws) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3.15->langchain-aws) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3.15->langchain-aws) (0.1.143)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3.15->langchain-aws) (24.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3.15->langchain-aws) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain-core<0.4,>=0.3.15->langchain-aws) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>=2->langchain-aws) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>=2->langchain-aws) (2.23.4)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.36.0,>=1.35.54->boto3>=1.34.131->langchain-aws) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from botocore<1.36.0,>=1.35.54->boto3>=1.34.131->langchain-aws) (2.2.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.15->langchain-aws) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-aws) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-aws) (3.10.11)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-aws) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-aws) (1.0.0)\n",
      "Requirement already satisfied: anyio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-aws) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-aws) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-aws) (1.0.6)\n",
      "Requirement already satisfied: idna in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-aws) (3.10)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-aws) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-aws) (0.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.54->boto3>=1.34.131->langchain-aws) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-aws) (3.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-aws) (1.2.2)\n",
      "Downloading langchain_aws-0.2.7-py3-none-any.whl (87 kB)\n",
      "Installing collected packages: langchain-aws\n",
      "Successfully installed langchain-aws-0.2.7\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "866356a1-558b-4536-a866-9979de7cb791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qa_df = pd.read_csv(\"qa_data.csv\")\n",
    "result_df = pd.read_csv(\"qa_with_mistral_responses.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7944f9c-57a6-4148-ae62-5d18c9181520",
   "metadata": {},
   "source": [
    "## Metrics: \n",
    "\n",
    "- `Faithfulness` metric measures the factual consistency of the generated answer against the given context. It is calculated from answer and retrieved context. Higher the better.\n",
    "- `FactualCorrectness` is a metric that compares and evaluates the factual accuracy of the generated response with the reference.\n",
    "- `Semantic Similarity` pertains to the assessment of the semantic resemblance between the generated answer and the ground truth. This evaluation is based on the ground truth and the answer, with values falling within the range of 0 to 1. A higher score signifies a better alignment between the generated answer and the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e26969db-dc73-4553-9c2f-3c5740c3ffa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import Faithfulness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "\n",
    "# AWS Bedrock configuration\n",
    "config = {\n",
    "    \"llm\": \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    \"embeddings\": \"amazon.titan-embed-text-v1\",\n",
    "    \"temperature\": 0.4,\n",
    "}\n",
    "\n",
    "# Initialize Bedrock evaluator models\n",
    "evaluator_llm = LangchainLLMWrapper(ChatBedrockConverse(\n",
    "    model=config[\"llm\"],\n",
    "    temperature=config[\"temperature\"],\n",
    "))\n",
    "\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(BedrockEmbeddings(\n",
    "    model_id=config[\"embeddings\"],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dd2bc1c-8abe-4bd2-affe-0f78740b59b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579fcfb162d1492e8da2e64ae0b668e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[4]: AttributeError('StringIO' object has no attribute 'statements')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'factual_correctness': 0.0312, 'faithfulness': 0.1342, 'semantic_similarity': 0.4264}\n"
     ]
    }
   ],
   "source": [
    "# Create dataset dictionary\n",
    "dataset_dict_1 = {\n",
    "    \"question\": qa_df['Question'].tolist()[:8],\n",
    "    \"answer\": result_df['mistral_response'].tolist()[:8],\n",
    "    \"ground_truth\": qa_df['Answer'].tolist()[:8],\n",
    "    \"retrieved_contexts\": [[context] if isinstance(context, str) else context \n",
    "                         for context in qa_df['Context'].tolist()[:8]]\n",
    "}\n",
    "ragas_dataset_1 = Dataset.from_dict(dataset_dict_1)\n",
    "\n",
    "metrics = [\n",
    "    FactualCorrectness(llm=evaluator_llm), \n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    SemanticSimilarity(embeddings=evaluator_embeddings),\n",
    "]\n",
    "\n",
    "results_mistral = evaluate(\n",
    "    dataset=ragas_dataset_1,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Evaluation Results:\", results_mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5afe4e-5ba9-48b3-bd02-9a2977d372a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_mistral.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db24c59e-cc8f-4e39-b7b6-2c73a1d2cd2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a4141986404549bf43ef1b0a86a956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'factual_correctness': 1.0000, 'faithfulness': 0.9531, 'semantic_similarity': 1.0000}\n"
     ]
    }
   ],
   "source": [
    "# Create dataset dictionary\n",
    "dataset_dict_2 = {\n",
    "    \"question\": qa_df['Question'].tolist()[:8],\n",
    "    \"answer\": qa_df['Answer'].tolist()[:8],\n",
    "    \"ground_truth\": qa_df['Answer'].tolist()[:8],\n",
    "    \"retrieved_contexts\": [[context] if isinstance(context, str) else context \n",
    "                         for context in qa_df['Context'].tolist()[:8]]\n",
    "}\n",
    "ragas_dataset_2 = Dataset.from_dict(dataset_dict_2)\n",
    "\n",
    "metrics = [\n",
    "    FactualCorrectness(llm=evaluator_llm), \n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    SemanticSimilarity(embeddings=evaluator_embeddings)\n",
    "]\n",
    "\n",
    "results_claude = evaluate(\n",
    "    dataset=ragas_dataset_2,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Evaluation Results:\", results_claude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ec144d-3a00-47f8-b545-2a435260378a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_claude.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efa7fcf-eabe-4311-aadd-dc6f5f728748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analysis(mistral_df, claude_df):\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 5))\n",
    "    \n",
    "    for i, col in enumerate(mistral_df.columns):\n",
    "        sns.kdeplot(\n",
    "            data=[mistral_df[col].values, claude_df[col].values],\n",
    "            legend=False,\n",
    "            ax=axs[i],\n",
    "            fill=True\n",
    "        )\n",
    "        axs[i].set_title(f'{col} scores distribution')\n",
    "        axs[i].legend(labels=[\"mistral\", \"claude\"])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the analysis function\n",
    "analysis(\n",
    "    results_mistral[['factual_correctness', 'faithfulness', 'semantic_similarity']],\n",
    "    results_mistral[['factual_correctness', 'faithfulness', 'semantic_similarity']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9048e7c7-7f6c-4fc7-aa9c-34bbeb2d6037",
   "metadata": {},
   "source": [
    "Obviously the results of comparing claude answers to claude answers will be perfect, this is just to test if the evaluation is working correctly. After, we will fill tune the mistral model and check again! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59154cc6-d122-4b5a-9a75-67d8e4614f6e",
   "metadata": {},
   "source": [
    "## Fine tune using PEFT - LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4890f04-c3f1-4c67-938a-9dce3c2c2438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1f361a1-d5d5-45ee-847e-aaac7c086bda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Question', 'Answer', 'Context'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"duartemoura/generathor_fine_tune\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d6c5166-5225-4ea7-beb8-01fa20f68d91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e6b04950ce42379c41bc014d797c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_dataset(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": example[\"Question\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"Answer\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Transform your dataset\n",
    "formatted_dataset = dataset.map(format_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb79f434-1e2c-4994-a997-edc000a5ea3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81bf95c46344a308cc52bb7e749f5a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "formatted_dataset = dataset.map(\n",
    "    format_dataset,\n",
    "    remove_columns=[\"Question\", \"Answer\", \"Context\"]  # Remove original fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9248c324-0a1a-42bc-9ce5-1fa074728eef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [{'content': 'What is the fundamental concept and process involved in nuclear medicine imaging?', 'role': 'user'}, {'content': 'Nuclear medicine imaging involves administering trace amounts of compounds labeled with radioactive isotopes (radionuclides) into the body. These radiolabeled compounds, called radiotracers or radiopharmaceuticals, emit gamma rays or high-energy photons when the radionuclide decays. External, position-sensitive gamma-ray detectors can detect these emissions and generate three-dimensional representations of the radiotracer distribution within the body, providing diagnostic information about various disease states.', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "print(formatted_dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74c5346b-ff60-439f-abc3-958725291e8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# define some variables - model names\n",
    "model_name = \"ministral/Ministral-3b-instruct\"\n",
    "new_model = \"ministral-ft\"\n",
    "\n",
    "################################################################################\n",
    "# LoRA parameters\n",
    "################################################################################\n",
    "# LoRA attention dimension\n",
    "# lora_r = 64\n",
    "lora_r = 4\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 25\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = 128 # None\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False # True\n",
    "# Load the entire model on the GPU 0\n",
    "# device_map = {\"\": 0}\n",
    "device_map=\"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310251db-b896-40a3-96cd-4028e4a95c1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ministral/Ministral-3b-instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Quantization config for bitsandbytes\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Use 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"ministral/Ministral-3b-instruct\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",  # LoRA task type\n",
    ")\n",
    "\n",
    "def tokenize_messages(example):\n",
    "    input_text = \"\"\n",
    "    for message in example[\"messages\"]:\n",
    "        input_text += f\"{message['role']}: {message['content']}\\n\"\n",
    "    return tokenizer(\n",
    "        input_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = formatted_dataset.map(tokenize_messages, batched=False)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    fp16=True,  # Use fp16 if supported\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    group_by_length=True,\n",
    "    warmup_ratio=0.03,\n",
    "    max_grad_norm=0.3,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=lora_config,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1670162f-5311-47a5-be5b-556d6f22889f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
